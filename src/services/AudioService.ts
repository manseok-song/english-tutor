/**
 * AudioService - Web Audio APIë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ìº¡ì²˜ ë° ì¬ìƒ
 * AudioWorklet ê¸°ë°˜ (ScriptProcessorNode deprecated ëŒ€ì²´)
 */

import { CONSTANTS } from '../types';

export interface VADState {
  isSpeaking: boolean;
  silenceFrames: number;
}

export class AudioService {
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private sourceNode: MediaStreamAudioSourceNode | null = null;
  private workletNode: AudioWorkletNode | null = null;
  private analyzerNode: AudioWorkletNode | null = null;
  private gainNode: GainNode | null = null;

  private playbackQueue: ArrayBuffer[] = [];
  private isPlaying = false;
  private isWorkletReady = false;

  // ì½œë°±
  public onAudioData: ((data: ArrayBuffer, isVoiceActive: boolean) => void) | null = null;
  public onAmplitudeChange: ((amplitude: number) => void) | null = null;
  public onPlaybackAmplitudeChange: ((amplitude: number) => void) | null = null;
  public onVADStateChange: ((speaking: boolean) => void) | null = null;

  // VAD ì„¤ì •
  private vadEnabled = true;
  private vadThreshold = 0.015;
  private silenceThreshold = 25;

  /**
   * AudioWorklet ì´ˆê¸°í™”
   */
  async initialize(): Promise<boolean> {
    try {
      // AudioContext ìƒì„± (iOS Safari í˜¸í™˜)
      const AudioContextClass = window.AudioContext ||
        (window as unknown as { webkitAudioContext: typeof AudioContext }).webkitAudioContext;

      this.audioContext = new AudioContextClass({
        sampleRate: CONSTANTS.INPUT_SAMPLE_RATE,
      });

      // iOSì—ì„œ AudioContext resume í•„ìš”
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }

      // AudioWorklet ëª¨ë“ˆ ë¡œë“œ
      try {
        await this.audioContext.audioWorklet.addModule('/audio-worklet-processor.js');
        this.isWorkletReady = true;
        console.log('âœ… AudioWorklet ì´ˆê¸°í™” ì™„ë£Œ');
      } catch (workletError) {
        console.warn('âš ï¸ AudioWorklet ë¡œë“œ ì‹¤íŒ¨, í´ë°± ëª¨ë“œ ì‚¬ìš©:', workletError);
        this.isWorkletReady = false;
      }

      return true;
    } catch (error) {
      console.error('âŒ AudioContext ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * VAD ì„¤ì • ë³€ê²½
   */
  setVADConfig(config: { enabled?: boolean; threshold?: number; silenceFrames?: number }): void {
    if (config.enabled !== undefined) this.vadEnabled = config.enabled;
    if (config.threshold !== undefined) this.vadThreshold = config.threshold;
    if (config.silenceFrames !== undefined) this.silenceThreshold = config.silenceFrames;

    // Workletì— ì„¤ì • ì „ë‹¬
    if (this.workletNode) {
      this.workletNode.port.postMessage({ type: 'setVadThreshold', value: this.vadThreshold });
      this.workletNode.port.postMessage({ type: 'setSilenceThreshold', value: this.silenceThreshold });
    }
  }

  /**
   * ë§ˆì´í¬ ìº¡ì²˜ ì‹œì‘
   */
  async startCapture(): Promise<boolean> {
    try {
      if (!this.audioContext) {
        const initialized = await this.initialize();
        if (!initialized) return false;
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: CONSTANTS.INPUT_SAMPLE_RATE,
        },
      });

      if (!this.audioContext) return false;

      // Resume context (iOS í•„ìˆ˜)
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }

      // ì†ŒìŠ¤ ë…¸ë“œ ìƒì„±
      this.sourceNode = this.audioContext.createMediaStreamSource(this.mediaStream);

      if (this.isWorkletReady) {
        // AudioWorklet ì‚¬ìš©
        await this.setupWorklet();
      } else {
        // í´ë°±: ScriptProcessorNode
        this.setupScriptProcessor();
      }

      console.log('ğŸ¤ ë§ˆì´í¬ ìº¡ì²˜ ì‹œì‘');
      return true;
    } catch (error) {
      console.error('âŒ ë§ˆì´í¬ ìº¡ì²˜ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * AudioWorklet ì„¤ì •
   */
  private async setupWorklet(): Promise<void> {
    if (!this.audioContext || !this.sourceNode) return;

    this.workletNode = new AudioWorkletNode(this.audioContext, 'audio-capture-processor');

    // Worklet ë©”ì‹œì§€ ìˆ˜ì‹ 
    this.workletNode.port.onmessage = (event) => {
      const { type, value, buffer, isVoiceActive, speaking } = event.data;

      switch (type) {
        case 'amplitude':
          this.onAmplitudeChange?.(value);
          break;
        case 'audioData':
          if (this.vadEnabled) {
            // VAD í™œì„±í™”: ìŒì„± í™œë™ ì‹œì—ë§Œ ì „ì†¡
            if (isVoiceActive) {
              this.onAudioData?.(buffer, true);
            }
          } else {
            // VAD ë¹„í™œì„±í™”: í•­ìƒ ì „ì†¡
            this.onAudioData?.(buffer, true);
          }
          break;
        case 'vadState':
          this.onVADStateChange?.(speaking);
          break;
      }
    };

    // VAD ì„¤ì • ì „ë‹¬
    this.workletNode.port.postMessage({ type: 'setVadThreshold', value: this.vadThreshold });
    this.workletNode.port.postMessage({ type: 'setSilenceThreshold', value: this.silenceThreshold });

    // ë…¸ë“œ ì—°ê²° (ì¶œë ¥ ì—†ì´ ë¶„ì„ë§Œ)
    this.sourceNode.connect(this.workletNode);
    // Workletì€ destinationì— ì—°ê²°í•˜ì§€ ì•ŠìŒ (ë§ˆì´í¬ í”¼ë“œë°± ë°©ì§€)
  }

  /**
   * í´ë°±: ScriptProcessorNode ì„¤ì • (deprecated but í˜¸í™˜ì„±ìš©)
   */
  private setupScriptProcessor(): void {
    if (!this.audioContext || !this.sourceNode) return;

    const processor = this.audioContext.createScriptProcessor(CONSTANTS.BUFFER_SIZE, 1, 1);
    let silenceFrames = 0;
    let isSpeaking = false;

    processor.onaudioprocess = (event) => {
      const inputData = event.inputBuffer.getChannelData(0);

      // RMS ê³„ì‚°
      let sum = 0;
      for (let i = 0; i < inputData.length; i++) {
        sum += inputData[i] * inputData[i];
      }
      const rms = Math.sqrt(sum / inputData.length);
      this.onAmplitudeChange?.(rms);

      // VAD
      const isVoiceActive = rms > this.vadThreshold;
      if (isVoiceActive) {
        silenceFrames = 0;
        if (!isSpeaking) {
          isSpeaking = true;
          this.onVADStateChange?.(true);
        }
      } else {
        silenceFrames++;
        if (isSpeaking && silenceFrames > this.silenceThreshold) {
          isSpeaking = false;
          this.onVADStateChange?.(false);
        }
      }

      // PCM ë³€í™˜ ë° ì „ì†¡
      if (!this.vadEnabled || isSpeaking || silenceFrames < 5) {
        const pcmData = this.float32ToInt16(inputData);
        this.onAudioData?.(pcmData.buffer, isVoiceActive);
      }
    };

    this.sourceNode.connect(processor);
    processor.connect(this.audioContext.destination);
  }

  /**
   * ë§ˆì´í¬ ìº¡ì²˜ ì¤‘ì§€
   */
  stopCapture(): void {
    if (this.workletNode) {
      this.workletNode.disconnect();
      this.workletNode = null;
    }

    if (this.sourceNode) {
      this.sourceNode.disconnect();
      this.sourceNode = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    console.log('ğŸ›‘ ë§ˆì´í¬ ìº¡ì²˜ ì¤‘ì§€');
  }

  /**
   * ì˜¤ë””ì˜¤ ì¬ìƒ (24kHz PCM16 -> ì¬ìƒ)
   */
  async playAudio(pcmData: ArrayBuffer): Promise<void> {
    if (!this.audioContext) {
      await this.initialize();
    }

    if (!this.audioContext) return;

    // Resume context (iOS í•„ìˆ˜)
    if (this.audioContext.state === 'suspended') {
      await this.audioContext.resume();
    }

    this.playbackQueue.push(pcmData);

    if (!this.isPlaying) {
      this.processPlaybackQueue();
    }
  }

  /**
   * ì¬ìƒ í ì²˜ë¦¬
   */
  private async processPlaybackQueue(): Promise<void> {
    if (!this.audioContext || this.playbackQueue.length === 0) {
      this.isPlaying = false;
      this.onPlaybackAmplitudeChange?.(0);
      return;
    }

    this.isPlaying = true;

    const pcmData = this.playbackQueue.shift()!;

    // Int16 -> Float32 ë³€í™˜
    const int16Array = new Int16Array(pcmData);
    const float32Array = new Float32Array(int16Array.length);

    for (let i = 0; i < int16Array.length; i++) {
      float32Array[i] = int16Array[i] / 32768;
    }

    // ì¬ìƒ ì¤‘ ì§„í­ ê³„ì‚°
    let sum = 0;
    for (let i = 0; i < float32Array.length; i++) {
      sum += float32Array[i] * float32Array[i];
    }
    const rms = Math.sqrt(sum / float32Array.length);
    this.onPlaybackAmplitudeChange?.(rms * 3); // ì¦í­

    // AudioBuffer ìƒì„± (24kHz)
    const audioBuffer = this.audioContext.createBuffer(
      1,
      float32Array.length,
      CONSTANTS.OUTPUT_SAMPLE_RATE
    );
    audioBuffer.copyToChannel(float32Array, 0);

    // ë²„í¼ ì†ŒìŠ¤ ìƒì„± ë° ì¬ìƒ
    const bufferSource = this.audioContext.createBufferSource();
    bufferSource.buffer = audioBuffer;

    if (!this.gainNode) {
      this.gainNode = this.audioContext.createGain();
      this.gainNode.gain.value = 1.0;
      this.gainNode.connect(this.audioContext.destination);
    }

    bufferSource.connect(this.gainNode);
    bufferSource.start();

    // ì¬ìƒ ì™„ë£Œ í›„ ë‹¤ìŒ ì²˜ë¦¬
    bufferSource.onended = () => {
      this.processPlaybackQueue();
    };
  }

  /**
   * ì¬ìƒ ì¤‘ì§€
   */
  stopPlayback(): void {
    this.playbackQueue = [];
    this.isPlaying = false;
    this.onPlaybackAmplitudeChange?.(0);
  }

  /**
   * ë³¼ë¥¨ ì¡°ì ˆ
   */
  setVolume(volume: number): void {
    if (this.gainNode) {
      this.gainNode.gain.value = Math.max(0, Math.min(1, volume));
    }
  }

  /**
   * Float32 -> Int16 PCM ë³€í™˜
   */
  private float32ToInt16(float32Array: Float32Array): Int16Array {
    const int16Array = new Int16Array(float32Array.length);

    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }

    return int16Array;
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  dispose(): void {
    this.stopCapture();
    this.stopPlayback();

    if (this.gainNode) {
      this.gainNode.disconnect();
      this.gainNode = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    console.log('ğŸ§¹ AudioService ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const audioService = new AudioService();
